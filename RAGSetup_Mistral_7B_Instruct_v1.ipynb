{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02a36bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers peft bitsandbytes accelerate pandas openpyxl datasets odfpy huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe14ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding, AutoConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a61b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c14570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31a9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e826a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6062e99737477d86409ecf46b4687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5934cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs_not_chat = tokenizer.encode_plus(\"[INST] Tell me about Georgia Tech ECE coursework [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "\n",
    "generated_ids = model.generate(inputs_not_chat, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc0d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> [INST] Tell me about Georgia Tech ECE coursework [/INST] Georgia Tech's Electrical and Computer Engineering (ECE) is a highly regarded program that offers a comprehensive curriculum in the fields of electrical and electronic systems. The ECE program covers everything from fundamental principles of electricity and magnetism to advanced topics in digital electronics, semiconductor devices, and computer networks.\\n\\nHere are some of the key areas of focus in Georgia Tech's ECE program:\\n\\n1. Electrical circuits and systems: This area covers topics such as voltage and current in circuits, circuit components, energy storage, and power systems.\\n2. Signal processing: This area provides an introduction to the analysis and manipulation of digital signals, including filtering, Fourier analysis, and digital signal processing algorithms.\\n3. Digital systems: This area covers topics such as binary logic and arithmetic, digital circuit design, and computer architecture.\\n4. Communications and networking: This area covers topics such as wireless and wired communications systems, data modulation and demodulation, and network protocols and architectures.\\n5. Computer systems: This area covers topics such as computer architecture, operating systems, compilers, and programming languages.\\n6. Electronics engineering: This area covers topics such as semiconductor devices, integrated circuits, microprocessors, and digital electronics.\\n7. Control: This area covers topics such as discrete-time and continuous-time systems, robust control, and nonlinear systems.\\n\\nIn addition to the core curriculum, Georgia Tech's ECE program also offers several specialized and interdisciplinary courses that allow students to tailor their studies to their specific interests and career goals. For example, students can choose to focus on areas such as renewable energy, cybersecurity, or computer vision.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78422f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c440dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (3.10.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (0.1.132)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Collecting sniffio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/hice1/bmallya3/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.1.3)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, jsonpointer, anyio\n",
      "Successfully installed anyio-4.6.2.post1 jsonpointer-3.0.0 sniffio-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9398163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# List of PDF file paths\n",
    "pdf_files = [\n",
    "    \"/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023-Spring-ECE8893-syllabus.pdf\",\n",
    "    \"/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023Spring-ECE8803ALT-Syllabus.pdf\",\n",
    "    \"/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2024-Spring-ECE8803_BFA-syllabus.pdf\",\n",
    "    \"/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2024Fall-ECE4803-8803EV-Syllabus.pdf\",\n",
    "    \"/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/ECE8803_WPS_Syllabus.pdf\"\n",
    "]\n",
    "\n",
    "# Load and process PDF files\n",
    "documents = []\n",
    "for pdf_path in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, \n",
    "                                               chunk_overlap=150,\n",
    "                                               length_function=len,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "db = FAISS.from_documents(chunked_documents, embeddings)\n",
    "\n",
    "# Connect query to FAISS index using a retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance\n",
    "    search_kwargs={'k': 5, 'fetch_k': 20}\n",
    ")\n",
    "\n",
    "# Optional: Save the FAISS index for later use\n",
    "db.save_local(\"FAISS_Vector_Database_ECE_Course_Syllabus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d2927c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "improving the design quality; and understand future trends and opportunities for FPGAs for a diverse range of applications such as GNNs, scientific computing, medical electronics, cybersecurity systems, and wireless communications.  Course Structure The course will involve a mix of lectures interspersed with heavy paper reading and discussions. A semester long programming-heavy project will focus on developing an FPGA accelerator using HLS for DNN or GNN algorithms.  Course Text The material for this course will be derived from the following texts: 1. Kastner, Ryan, Janarbek Matai, and Stephen Neuendorffer. \"Parallel programming for FPGAs.\" arXiv preprint arXiv:1805.03648 (2018).\n",
      "---\n",
      "Result 2:\n",
      "handled by the office of student affairs. Students will have to do all assignments individually unless explicitly told otherwise. Students may discuss with classmates but may not copy any solution (or any part of a solution).\n",
      "---\n",
      "Result 3:\n",
      "important to strive for an atmosphere of mutual respect and responsibility between faculty members and students. In the end, a respect for knowledge and understanding, an appreciation for hard work, and respectful interactions all contribute to an environment conducive to learning and excellence. I encourage you to remain committed to the ideals of Georgia Tech while in this class. See www.catalog.gatech.edu/rules/22 for a description of some basic expectations that we can have of each other.\n",
      "---\n",
      "Result 4:\n",
      "The class will follow the Institute absence policy detailed at \n",
      "http://www.catalog.gatech.edu/rules/4/  \n",
      "Student -Faculty Expectations Agreement  \n",
      "At Georgia Tech we believe that it is important to strive for an a tmosphere of mutual respect, \n",
      "acknowledgement, and responsibility betw een faculty members and the student body. See \n",
      "http://www.catalog.gatech.edu/rules/22/   \n",
      "Course Outline  \n",
      " \n",
      "a) Electron dynamics  \n",
      "- Landauer Transport Equation  \n",
      "- Boltzmann Transport Equation (BTE)  \n",
      "- Solving the BTE in equilibrium  \n",
      "- Evaluating moments of the distribution function  \n",
      " \n",
      "b) Off-equilibrium transport in devices  \n",
      "- Simplified approximations to carrier distribution function (displaced Maxwellian)  \n",
      "- Energy relaxation mechanisms  \n",
      " c) Ballistic and quantum ballistic transport  \n",
      "- Solving the ballistic BTE  \n",
      "- Resistance of a ballistic conductor  \n",
      "- Quasi -ballistic transport  \n",
      "- Example s: ultra -short channel MOSFET, quantum well FET, cryogenic MOSFET \n",
      " d) Collision -dominated transport  \n",
      "- Relaxation time approximation  \n",
      "- Carrier scattering in bulk and in low -dimensional systems  \n",
      "- Low-field mobility, conductivity mass tensor  \n",
      "- High -field transport  \n",
      "- Monte Carlo simulation of transport  \n",
      " \n",
      "e) Extremely scaled MOSFETs \n",
      "- Scaling theory  \n",
      "- Virtual source model: above and below threshold  \n",
      "- Virtual source model: quasi ballistic and ballistic  \n",
      "- Leakage  \n",
      "- Parasitic Resistance and Capacitance  \n",
      "- Variation  \n",
      "- Reliability\n",
      "---\n",
      "Result 5:\n",
      "EV Course Schedule (Sp2024)  Class Date Lecture Topic Homework Roadmap  8/19 1 Introduction to the course (ICE cars, e-mobility, hybrid EVs, driving range, efficiency, limitations, and the need for EVs) Part 1    8/21 2 Introduction to the course (ICE, e-mobility, hybrid EVs, driving range, limitations, and the need for and EVs) Part 2   8/26 3 Drivetrain fundamentals (single-speed and two-speed transmissions, single-motor and double-motor configurations, configurations including forward, rear, all and four-wheel drives, gears, advantages and disadvantages, regenerative braking) Part 1    8/28 4 Drivetrain fundamentals (single-speed and two- EV drivetrain fundamentals (single-speed and two-speed transmissions, configurations including forward, rear, all and four-wheel drives, gears, advantages and disadvantages, regenerative braking) Part 2  9/2 -  Labor Day  Assignment 1  9/4 5 Battery (different types and electro-chemistries, characteristics, modules and packs, state-of-charge estimation, C-rates, lifetime, cost, efficiency, cell balancing, limitations, 400 V vs. 800 V battery systems, and thermal and power management) Part 1  9/9 6 Batteries (different types and electro-chemistries, characteristics, modules and packs, state-of-charge estimation, C-rates, lifetime, cost, efficiency, cell balancing, limitations, 400 V vs. 800 V battery systems, and thermal and power management) Part 2   9/11 7 Batteries (different types and electro-chemistries, characteristics,\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Who is the course instructor for Parallel Programming for FPGAs?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d7dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.3,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "ECE Coursework knowledge. If you don't know the answer to a question, say \"I don't know\". Here is the context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74a499f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': '',\n",
       " 'question': 'Who is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech?',\n",
       " 'text': '\\n### [INST] \\nInstruction: Answer the question based on your \\nECE Coursework knowledge. If you don\\'t know the answer to a question, say \"I don\\'t know\". Here is the context to help:\\n\\n\\n\\n### QUESTION:\\nWho is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech? \\n\\n[/INST]\\n  I don\\'t have access to current information about specific courses or instructors at Georgia Tech. However, you can check the official Georgia Tech website or contact their academic department for the most up-to-date information.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"context\":\"\", \n",
    "                  \"question\": \"Who is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8797a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': '/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023Spring-ECE8803ALT-Syllabus.pdf', 'page': 0}, page_content='ECE  8803 ALT  Advanced Logic Transistor: Physics and Technology (Spring  2023) \\nUnit: 3 \\nTime: Mon , Wed 1400- 1515   \\nLocation: 2456  Klaus Advanced Comput ing  \\nInstructor: Dr. Suman Datta , Joseph M Pettit Chair in Advanced Computing and Professor of \\nElectrical  and Computer  Engineering, Georgia Institute of Technology   \\nOffice:  Klaus 2360  \\nEmail: sdatta68 @ece.gatech.edu  \\nPhone:  404-894-6738  \\nOffice Hour s: Wed 1 700-1850 \\nCourse Objectives:  \\nTo develop fundamental understanding of  scattering limited transport versus ballistic tran sport of \\ncarriers in modern semiconductor devices, to quantitively analyze the electrostatic robustness in \\nultra-scaled MOS transistors and to introduce graduate students specializing in semiconductor \\nscience and technology  to the process of identifying the  mechanisms which limit the modern -day \\ntransistor  performance, both intrinsic and extrinsic, and seed new ideas for future improvement  of \\ntransistors from the standpoint  of scaling, performance and energy  efficiency . \\nProspective S tudents:  \\nGraduate students in electrical  and computer engineering program as well as in materials science \\nand engineering program are welcome to take the course.  Students major ing in TIGs of \\nnanotechnology and VLSI systems and digital design who are interested in t he physics and \\ntechnology of advanced CMOS logic  will be interested in  taking  this course to get acquainted'),\n",
       "  Document(metadata={'source': '/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023-Spring-ECE8893-syllabus.pdf', 'page': 0}, page_content='improving the design quality; and understand future trends and opportunities for FPGAs for a diverse range of applications such as GNNs, scientific computing, medical electronics, cybersecurity systems, and wireless communications.  Course Structure The course will involve a mix of lectures interspersed with heavy paper reading and discussions. A semester long programming-heavy project will focus on developing an FPGA accelerator using HLS for DNN or GNN algorithms.  Course Text The material for this course will be derived from the following texts: 1. Kastner, Ryan, Janarbek Matai, and Stephen Neuendorffer. \"Parallel programming for FPGAs.\" arXiv preprint arXiv:1805.03648 (2018).'),\n",
       "  Document(metadata={'source': '/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023Spring-ECE8803ALT-Syllabus.pdf', 'page': 1}, page_content='Prerequisites \\nUndergraduate -level knowledge about semiconductor device s are required .   \\nAssignments and Grading:  \\nHomework 50%, 5 problem sets ( 10% each).  Pre-knowledge about numerical tools such as \\nMATLAB is required.  \\nMidterm Exam (Mid -semester ) 25%, Open  book and Open  lecture notes, one cheat sheet and a \\ncalculator allowed.  \\nFinal Exam  (Final Exam Period) 25% . Open  book and Open  lecture notes, one cheat sheet and a \\ncalculator allowed.  \\nExpected grade distribution: 85+ is A, 7 0+ is B, 55+ is C, 40+ is D, <40 is F .  \\nNo official textbook required, optional  reference s are   \\n1. S. Datta “Electronic Transport in Mesoscopic Systems”,  \\n2. Mark Lundstrom and Jing Guo “Nanoscale Transistors: Device Physics, Modeling and \\nSimulation” Published by Springer . \\nNotes/ Policy  \\n• If you  turn in your homework after the deadline, your score will be multiplied  by a factor of \\n80% for one day late,  50% for two days late, and no score beyond three days late.  \\n• Submit homework in the electronic ally scanned version.  \\n• Questions regarding homework/exam grading must be asked within one week after the \\nhomework/exam is returned . \\n \\nAcademic Honor Code \\nThe Honor Code applies to every aspect of this class, with only one noteworthy exception: \\nstudent discussion of concepts and techniques for solving homework problems i s permitted \\noutside the classroom. However, all the submitted work must be original  and by individual . More'),\n",
       "  Document(metadata={'source': '/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2024Fall-ECE4803-8803EV-Syllabus.pdf', 'page': 2}, page_content='important to strive for an atmosphere of mutual respect and responsibility between faculty members and students. In the end, a respect for knowledge and understanding, an appreciation for hard work, and respectful interactions all contribute to an environment conducive to learning and excellence. I encourage you to remain committed to the ideals of Georgia Tech while in this class. See www.catalog.gatech.edu/rules/22 for a description of some basic expectations that we can have of each other.')],\n",
       " 'question': 'Who is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech??',\n",
       " 'text': '\\n### [INST] \\nInstruction: Answer the question based on your \\nECE Coursework knowledge. If you don\\'t know the answer to a question, say \"I don\\'t know\". Here is the context to help:\\n\\n[Document(metadata={\\'source\\': \\'/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023Spring-ECE8803ALT-Syllabus.pdf\\', \\'page\\': 0}, page_content=\\'ECE  8803 ALT  Advanced Logic Transistor: Physics and Technology (Spring  2023) \\\\nUnit: 3 \\\\nTime: Mon , Wed 1400- 1515   \\\\nLocation: 2456  Klaus Advanced Comput ing  \\\\nInstructor: Dr. Suman Datta , Joseph M Pettit Chair in Advanced Computing and Professor of \\\\nElectrical  and Computer  Engineering, Georgia Institute of Technology   \\\\nOffice:  Klaus 2360  \\\\nEmail: sdatta68 @ece.gatech.edu  \\\\nPhone:  404-894-6738  \\\\nOffice Hour s: Wed 1 700-1850 \\\\nCourse Objectives:  \\\\nTo develop fundamental understanding of  scattering limited transport versus ballistic tran sport of \\\\ncarriers in modern semiconductor devices, to quantitively analyze the electrostatic robustness in \\\\nultra-scaled MOS transistors and to introduce graduate students specializing in semiconductor \\\\nscience and technology  to the process of identifying the  mechanisms which limit the modern -day \\\\ntransistor  performance, both intrinsic and extrinsic, and seed new ideas for future improvement  of \\\\ntransistors from the standpoint  of scaling, performance and energy  efficiency . \\\\nProspective S tudents:  \\\\nGraduate students in electrical  and computer engineering program as well as in materials science \\\\nand engineering program are welcome to take the course.  Students major ing in TIGs of \\\\nnanotechnology and VLSI systems and digital design who are interested in t he physics and \\\\ntechnology of advanced CMOS logic  will be interested in  taking  this course to get acquainted\\'), Document(metadata={\\'source\\': \\'/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023-Spring-ECE8893-syllabus.pdf\\', \\'page\\': 0}, page_content=\\'improving the design quality; and understand future trends and opportunities for FPGAs for a diverse range of applications such as GNNs, scientific computing, medical electronics, cybersecurity systems, and wireless communications.  Course Structure The course will involve a mix of lectures interspersed with heavy paper reading and discussions. A semester long programming-heavy project will focus on developing an FPGA accelerator using HLS for DNN or GNN algorithms.  Course Text The material for this course will be derived from the following texts: 1. Kastner, Ryan, Janarbek Matai, and Stephen Neuendorffer. \"Parallel programming for FPGAs.\" arXiv preprint arXiv:1805.03648 (2018).\\'), Document(metadata={\\'source\\': \\'/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2023Spring-ECE8803ALT-Syllabus.pdf\\', \\'page\\': 1}, page_content=\\'Prerequisites \\\\nUndergraduate -level knowledge about semiconductor device s are required .   \\\\nAssignments and Grading:  \\\\nHomework 50%, 5 problem sets ( 10% each).  Pre-knowledge about numerical tools such as \\\\nMATLAB is required.  \\\\nMidterm Exam (Mid -semester ) 25%, Open  book and Open  lecture notes, one cheat sheet and a \\\\ncalculator allowed.  \\\\nFinal Exam  (Final Exam Period) 25% . Open  book and Open  lecture notes, one cheat sheet and a \\\\ncalculator allowed.  \\\\nExpected grade distribution: 85+ is A, 7 0+ is B, 55+ is C, 40+ is D, <40 is F .  \\\\nNo official textbook required, optional  reference s are   \\\\n1. S. Datta “Electronic Transport in Mesoscopic Systems”,  \\\\n2. Mark Lundstrom and Jing Guo “Nanoscale Transistors: Device Physics, Modeling and \\\\nSimulation” Published by Springer . \\\\nNotes/ Policy  \\\\n• If you  turn in your homework after the deadline, your score will be multiplied  by a factor of \\\\n80% for one day late,  50% for two days late, and no score beyond three days late.  \\\\n• Submit homework in the electronic ally scanned version.  \\\\n• Questions regarding homework/exam grading must be asked within one week after the \\\\nhomework/exam is returned . \\\\n \\\\nAcademic Honor Code \\\\nThe Honor Code applies to every aspect of this class, with only one noteworthy exception: \\\\nstudent discussion of concepts and techniques for solving homework problems i s permitted \\\\noutside the classroom. However, all the submitted work must be original  and by individual . More\\'), Document(metadata={\\'source\\': \\'/home/hice1/bmallya3/scratch/ECE_Special_Topics_Syllabus_PDFs/2024Fall-ECE4803-8803EV-Syllabus.pdf\\', \\'page\\': 2}, page_content=\\'important to strive for an atmosphere of mutual respect and responsibility between faculty members and students. In the end, a respect for knowledge and understanding, an appreciation for hard work, and respectful interactions all contribute to an environment conducive to learning and excellence. I encourage you to remain committed to the ideals of Georgia Tech while in this class. See www.catalog.gatech.edu/rules/22 for a description of some basic expectations that we can have of each other.\\')]\\n\\n### QUESTION:\\nWho is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech?? \\n\\n[/INST]\\n \\nThe course instructor for ECE Advanced Logic Transistor course from Georgia Tech is Dr. Suman Datta.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "query = \"Who is the course instructor for ECE Advanced Logic Transistor course from Georgia Tech??\" \n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc8aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad45057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05554d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c65dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f50a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be34199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc215ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf708aaa7b54c19bfa4d089fcb05e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"Query_Response_Pairs_ConvAI_Project.csv\", encoding='latin-1')\n",
    "\n",
    "# def prepare_data(row):\n",
    "#     return f\"### Human: {row['Query']}\\n\\n### Assistant: {row['Response']}\\n\\n\"\n",
    "\n",
    "# # Create a new DataFrame with the prepared data\n",
    "# prepared_df = pd.DataFrame({\n",
    "#     'text': df.apply(prepare_data, axis=1)\n",
    "# })\n",
    "\n",
    "# # Create the dataset\n",
    "# dataset = Dataset.from_pandas(prepared_df)\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfc9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fa5085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ca2395f6749bda5fe8b318a42dce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"Mistral-7B-v0.1\",quantization_config=bnb_config,\n",
    "#     device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c8bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a14578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf9fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf8ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     warmup_steps=100,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,  # Enable mixed precision training\n",
    "#     remove_unused_columns=False,\n",
    "#     no_cuda=False,  # Ensure CUDA is used if available\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b713a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e2934a629c4f1dbf01f79a879426d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Mistral-7B-v0.1\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b26179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "# from typing import Any, Dict, List, Union, Tuple\n",
    "# import torch\n",
    "\n",
    "# @dataclass\n",
    "# class CustomDataCollatorForLanguageModeling:\n",
    "#     tokenizer: PreTrainedTokenizerBase\n",
    "#     mlm: bool = False\n",
    "#     mlm_probability: float = 0.15\n",
    "\n",
    "#     def __call__(self, examples: List[Dict[str, Union[List[int], Any]]]) -> Dict[str, torch.Tensor]:\n",
    "#         batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n",
    "        \n",
    "#         if \"label\" in batch:\n",
    "#             batch[\"labels\"] = batch[\"label\"]\n",
    "#             del batch[\"label\"]\n",
    "#         elif \"labels\" not in batch:\n",
    "#             batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "\n",
    "#         if self.mlm:\n",
    "#             batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(batch[\"input_ids\"])\n",
    "\n",
    "#         return batch\n",
    "\n",
    "#     def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         # Implement masking logic here if needed\n",
    "#         return inputs, inputs.clone()\n",
    "\n",
    "# data_collator = CustomDataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c887813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     data_collator=data_collator,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0487da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 03:27, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hice1/bmallya3/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108, training_loss=1.6492347695209362, metrics={'train_runtime': 209.9344, 'train_samples_per_second': 8.345, 'train_steps_per_second': 0.514, 'total_flos': 3.778252262866944e+16, 'train_loss': 1.6492347695209362, 'epoch': 2.958904109589041})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b7e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./fine_tuned_mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99d1ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7523c2d751aa4f5883f34ba7b637cba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel, PeftConfig\n",
    "\n",
    "# # Load the fine-tuned model configuration\n",
    "# config = PeftConfig.from_pretrained(\"./fine_tuned_mistral\")\n",
    "\n",
    "# # Load the tokenizer and set the pad token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "# model = PeftModel.from_pretrained(model, \"./fine_tuned_mistral\")\n",
    "\n",
    "# # Set the pad token ID in the model config\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d6b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What are the key learning outcomes of ECE 8803 HOS?\n",
      "\n",
      "### Assistant: After completing ECE 8803 HOS, students will be able to:\n",
      "1. Describe the fundamental concepts of heterogeneous computing systems, including multicore, GPU, and FPGA architectures.\n",
      "2. Design and implement high-performance heterogeneous computing systems using CUDA, OpenCL, or other relevant programming frameworks.\n",
      "3. Analyze and optimize performance and energy efficiency of heterogeneous computing systems using profiling and debugging tools.\n",
      "4. Evaluate the trade-offs between performance, energy efficiency, and power consumption in heterogeneous computing systems.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# def generate_response(query):\n",
    "#     input_text = f\"### Human: {query}\\n\\n### Assistant:\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Move inputs to GPU if available\n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "#         model.to('cuda')\n",
    "    \n",
    "#     try:\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_length=200,\n",
    "#                 num_return_sequences=1,\n",
    "#                 do_sample=True,\n",
    "#                 temperature=0.7,\n",
    "#                 top_p=0.9\n",
    "#             )\n",
    "#         return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     except Exception as e:\n",
    "#         return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# # Example usage\n",
    "# query = \"What are the key learning outcomes of ECE 8803 HOS?\"\n",
    "# response = generate_response(query)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c669b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ce41bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699c6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
